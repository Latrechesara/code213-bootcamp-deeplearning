{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "efb4360f",
   "metadata": {},
   "source": [
    "# Reinforcement Learning Lab: Tic-Tac-Toe with QLearning\n",
    "<img src=\"../images/tictactoe.png\" alt=\"Tic Tac Toe Example\" width=\"200\"/>\n",
    "\n",
    "**What is this lab about?**  \n",
    "\n",
    "In this lab, weâ€™ll train an **RL agent** to play **Tic-Tac-Toe** using the **Qlearningalgorithm**.  \n",
    "SARSA is an **on-policy** algorithm, meaning it updates Q-values using the action actually taken, not the best future action.  \n",
    "\n",
    "We will focus on:  \n",
    "\n",
    "- Understanding the **QLearning update rule**.  \n",
    "- Implementing the agent.  \n",
    "- Training against a random opponent.  \n",
    "- Visualizing results.  \n",
    "\n",
    "---\n",
    "\n",
    "## Table of Contents  \n",
    "\n",
    "- [1 - Packages](#1)  \n",
    "- [2 - Tic-Tac-Toe Environment](#2)  \n",
    "- [3 - SARSA Agent](#3)  \n",
    "- [4 - Training Loop](#4)  \n",
    "- [5 - Plotting Results](#5)  \n",
    "- [6 - Evaluation](#6)  \n",
    "- [7 - Exercises](#7)  \n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51fe693f",
   "metadata": {},
   "source": [
    "# <a name='1'></a>\n",
    "## 1 - Packages\n",
    "\n",
    "We start by importing the necessary Python libraries.  \n",
    "\n",
    "- **numpy** â†’ for handling states and arrays.  \n",
    "- **random** â†’ for exploration and opponentâ€™s random moves.  \n",
    "- **matplotlib** â†’ for plotting training results later.  \n",
    "- **collections** â†’ for tracking performance statistics (wins, draws, losses).  \n",
    "\n",
    "These will be the main dependencies throughout the notebook.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "54b447a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict, deque\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c12cae0",
   "metadata": {},
   "source": [
    "# <a name='2'></a>\n",
    "## 2 - Tic-Tac-Toe Environment  \n",
    "\n",
    "We now create the **game environment** where our RL agent will play.  \n",
    "\n",
    "### Board Representation  \n",
    "- The board is a flat array of 9 cells.  \n",
    "- Values:  \n",
    "  - `0` â†’ empty cell  \n",
    "  - `1` â†’ agent (X)  \n",
    "  - `-1` â†’ opponent (O)  \n",
    "\n",
    "<img src=\"../images/tictactoe-board.png\" width=\"250\"/>  \n",
    "\n",
    "### Environment Mechanics  \n",
    "- `reset()` â†’ clears the board.  \n",
    "- `available_actions()` â†’ returns indices of empty cells.  \n",
    "- `step(action, player)` â†’ executes a move, returns `(new_state, reward, done)`.  \n",
    "- `check_winner()` â†’ checks rows, columns, diagonals.  \n",
    "\n",
    "### Rewards  \n",
    "- `+1` â†’ agent wins  \n",
    "- `-1` â†’ agent loses  \n",
    "- `0` â†’ draw  \n",
    "- `-10` â†’ illegal move (placing in occupied cell)  \n",
    "\n",
    "This environment will be used for both Q-Learning and SARSA.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "895a8279",
   "metadata": {},
   "source": [
    "<a name='4'></a>\n",
    "## 4 - Running Episodes  \n",
    "\n",
    "Now we combine the **environment** and the **Q-learning agent** to simulate games.  \n",
    "\n",
    "### Training Loop  \n",
    "\n",
    "At each episode:  \n",
    "1. Reset the environment to get the initial state.  \n",
    "2. Let the agent play until the game ends (`done=True`).  \n",
    "3. At each step:  \n",
    "   - Choose action $a$ using $\\epsilon$-greedy.  \n",
    "   - Apply action â†’ get $(s', r, \\text{done})$.  \n",
    "   - Update $Q(s,a)$ using the Q-learning update rule.  \n",
    "4. Track results: win, loss, or draw.  \n",
    "\n",
    "<img src=\"../images/episode-loop.png\" width=\"400\"/>  \n",
    "\n",
    "This loop allows the agent to **learn from repeated games** and gradually improve.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "312ea104",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "32df424e",
   "metadata": {},
   "source": [
    "### Test Run  \n",
    "\n",
    "Letâ€™s train the agent for a few episodes and see if it starts to improve.  \n",
    "\n",
    "Weâ€™ll:  \n",
    "- Train for 100 episodes.  \n",
    "- Print how many games the agent **won**.  \n",
    "- Later, weâ€™ll visualize the results with plots.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa55d7bc",
   "metadata": {},
   "source": [
    "<a name='5'></a>\n",
    "## 5 - Plotting Results  \n",
    "\n",
    "To understand how our agent is improving, we need to **visualize the training progress**.  \n",
    "\n",
    "Weâ€™ll plot:  \n",
    "- **Rewards per Episode** â†’ shows how the agentâ€™s returns evolve.  \n",
    "- **Rolling Average Rewards** â†’ smooths out randomness and shows overall trends.  \n",
    "- **Win Rate Curve** â†’ how often the agent wins as training progresses.  \n",
    "\n",
    "These plots help us verify if the Q-learning agent is actually **learning** or just playing randomly.  \n",
    "\n",
    "<img src=\"../images/qlearning-training-curve.png\" width=\"400\"/>  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3b2fdb8",
   "metadata": {},
   "source": [
    "<a name='6'></a>\n",
    "## 6 - Exercises ðŸŽ¯  \n",
    "\n",
    "Now that you have built and trained a **Q-Learning agent** for Tic-Tac-Toe, try the following extensions:\n",
    "\n",
    "1. **Vary Hyperparameters**  \n",
    "   - Change $\\alpha$, $\\gamma$, or $\\epsilon$ and see how training performance changes.  \n",
    "   - Plot and compare learning curves.  \n",
    "\n",
    "2. **Play Against the Agent**  \n",
    "   - Implement a simple text-based or GUI interface.  \n",
    "   - Try playing against your trained agent. Can you beat it?  \n",
    "\n",
    "3. **Implement SARSA**  \n",
    "   - Replace the Q-learning update with the SARSA update:  \n",
    "   $$\n",
    "   Q(s,a) \\gets Q(s,a) + \\alpha \\Big[ r + \\gamma Q(s',a') - Q(s,a) \\Big]\n",
    "   $$  \n",
    "   - Compare results with Q-Learning.  \n",
    "\n",
    "4. **Add a Deep Q-Network (DQN)**  \n",
    "   - Replace the Q-table with a neural network.  \n",
    "   - Train it using the same environment.  \n",
    "\n",
    "5. **Experiment with Exploration**  \n",
    "   - Try different exploration strategies (e.g., decaying $\\epsilon$, softmax).  \n",
    "   - Observe how agent behavior changes.  \n",
    "\n",
    "6. **Deployment Idea ðŸ’¡**  \n",
    "   - Wrap your trained agent in an API.  \n",
    "   - Build a small GUI so humans can play against it.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0a9aad4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
