{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "45e54d5e",
   "metadata": {},
   "source": [
    "#  Reinforcement Learning Lab: Snake Game & Exploration vs Exploitation  \n",
    "<img src=\"../images/SnakeMaze.png\" alt=\"Snake Game Example\" width=\"250\"/>\n",
    "\n",
    "\n",
    "**What is this lab about?**  \n",
    "\n",
    "In this lab, we’ll explore how a Reinforcement Learning (RL) agent can learn to play the **Snake game**.  \n",
    "We will focus on the classic RL dilemma:  \n",
    "\n",
    "- **Exploration** : trying new moves, even if they might fail.  \n",
    "- **Exploitation** : choosing the best-known move to maximize score.  \n",
    "\n",
    "This lab connects directly to the **$\\epsilon$-greedy policy**.  \n",
    "By adjusting $\\epsilon$, we can control how much the agent explores versus exploits.  \n",
    "\n",
    "\n",
    "\n",
    "## Table of Contents  \n",
    "\n",
    "- [1 - Packages](#1)  \n",
    "- [2 - Minimal Snake Environment](#2)  \n",
    "- [3 - Agent with $\\epsilon$-Greedy Policy](#3)  \n",
    "- [4 - Running Episodes](#4)  \n",
    "- [5 - Plotting Results](#5)  \n",
    "- [6 - Exercises](#6)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecd85392",
   "metadata": {},
   "source": [
    "<a name='1'></a>\n",
    "## 1 - Packages\n",
    "In this section, we import the core Python libraries needed for our lab, including `numpy`, `matplotlib`, and the necessary RL environment setup.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "63e53683",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named '_____'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# === YOUR CODE HERE ===\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m_____\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m_____\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mplt\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named '_____'"
     ]
    }
   ],
   "source": [
    "# === YOUR CODE HERE ===\n",
    "import _____ as np\n",
    "import _____\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eeae317",
   "metadata": {},
   "source": [
    "<a name='2'></a>\n",
    "## 2 - Minimal Snake Environment  \n",
    "We represent the Snake game on an $8 \\times 8$ grid.  \n",
    "\n",
    "<img src=\"../images/snake-grid.png\" width=\"300\"/>  \n",
    "\n",
    "### How the Environment Updates  \n",
    "\n",
    "<img src=\"../images/snake-env-diagram.png\" width=\"400\"/>  \n",
    "\n",
    "- Snake moves based on actions (`up`, `down`, `left`, `right`).  \n",
    "- If the snake eats food → reward = +10.  \n",
    "- If it crashes into wall/body → reward = -10 (game over).  \n",
    "- Otherwise → reward = -0.1 (penalty for wasting time). \n",
    "\n",
    "The snake starts in the top-left corner, and food is randomly placed.  \n",
    "We’ll build a very simple **Snake environment**:  \n",
    "- The snake starts at position `(0,0)`.  \n",
    "- Food is randomly placed on the grid.  \n",
    "- Actions: 0 = up, 1 = down, 2 = left, 3 = right.  \n",
    "- Rewards: +10 for eating food, -10 for dying, small negative step otherwise.  \n",
    "\n",
    "**Exercise:** Implement the `SnakeEnv` class with:  \n",
    "- `reset()` → resets snake and food.  \n",
    "- `get_state()` → returns head + food positions.  \n",
    "- `step(action)` → updates state and returns `(state, reward, done)`.  \n",
    "\n",
    " *Hint: Use a list `self.snake` to store snake body segments.*  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9484fa65",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SnakeEnv:\n",
    "    GRID_SIZE = 8\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "    \n",
    "    def reset(self):\n",
    "        # === YOUR CODE HERE ===\n",
    "        self.snake = [(0,0)]\n",
    "        self.food = (np.random.randint(0,self.GRID_SIZE),\n",
    "                     np.random.randint(0,self.GRID_SIZE))\n",
    "        self.done = False\n",
    "        self.score = 0\n",
    "        return self.get_state()\n",
    "    \n",
    "    def get_state(self):\n",
    "        # return head + food positions as numpy array\n",
    "        head = self.snake[0]\n",
    "        return np.array([head[0], head[1], self.food[0], self.food[1]])\n",
    "    \n",
    "    def step(self, action):\n",
    "        # === STUDENT TODO === implement movement + reward rules\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cadd2d1",
   "metadata": {},
   "source": [
    "<a name='3'></a>\n",
    "## 3 - Agent with $\\epsilon$-Greedy Policy  \n",
    "The agent balances **exploration vs exploitation**:  \n",
    "\n",
    "<img src=\"../images/epsilon-greedy.png\" width=\"400\"/>  \n",
    "\n",
    "- With probability **ε**, choose a random action (exploration).  \n",
    "- With probability **1-ε**, choose the best action (exploitation)\n",
    "\n",
    "We now create an **agent** that:  \n",
    "- Picks a **random action** with probability $\\epsilon$.  \n",
    "- Picks the **best action** (argmax) with probability $1-\\epsilon$.  \n",
    "- Decays $\\epsilon$ after each episode.  \n",
    "\n",
    "**Exercise:** Implement the `Agent` class with:  \n",
    "- `get_action(q_values)` → returns chosen action.  \n",
    "- `decay_epsilon()` → decreases $\\epsilon$ each episode.  \n",
    "\n",
    " *Hint: Use `random.random()` to compare against epsilon.*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4977329a",
   "metadata": {},
   "source": [
    "class Agent:\n",
    "    def __init__(self, n_actions=4, epsilon=1.0, epsilon_min=0.01, decay=0.995):\n",
    "        self.n_actions = n_actions\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_min = epsilon_min\n",
    "        self.decay = decay\n",
    "    \n",
    "    def get_action(self, q_values):\n",
    "        # === YOUR CODE HERE ===\n",
    "        pass\n",
    "    \n",
    "    def decay_epsilon(self):\n",
    "        # === YOUR CODE HERE ===\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23956082",
   "metadata": {},
   "source": [
    "<a name='4'></a>\n",
    "## 4 - Running Episodes  \n",
    "\n",
    "Now, let’s simulate games of Snake.  \n",
    "\n",
    "**Steps:**  \n",
    "1. Reset the environment.  \n",
    "2. While not done:  \n",
    "   - Choose an action with the agent.  \n",
    "   - Step the environment.  \n",
    "   - Accumulate reward.  \n",
    "3. After each episode, decay epsilon.  \n",
    "\n",
    "**Exercise:** Write the training loop for 50 episodes.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1b2819a",
   "metadata": {},
   "source": [
    "env = SnakeEnv()\n",
    "agent = Agent()\n",
    "\n",
    "scores = []\n",
    "epsilons = []\n",
    "\n",
    "for episode in range(50):\n",
    "    state = env.reset()\n",
    "    total_reward = 0\n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        q_values = np.random.random(4)  # simulate Q-values\n",
    "        action = agent.get_action(q_values)\n",
    "        next_state, reward, done = env.step(action)\n",
    "        \n",
    "        # === YOUR CODE HERE ===\n",
    "        total_reward += reward\n",
    "        state = next_state\n",
    "    \n",
    "    agent.decay_epsilon()\n",
    "    scores.append(total_reward)\n",
    "    epsilons.append(agent.epsilon)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e8cba64",
   "metadata": {},
   "source": [
    "<a name='5'></a>\n",
    "## 5 - Plotting Results  \n",
    "### Epsilon Decay  \n",
    "\n",
    "We gradually reduce ε over time to shift from exploration to exploitation:  \n",
    "\n",
    "<img src=\"../images/epsilon-decay-curve.png\" width=\"400\"/> \n",
    "\n",
    "We want to see:  \n",
    "- **Episode rewards** (how well the agent performed).  \n",
    "- **Exploration rate $\\epsilon$** (how it decayed).  \n",
    "\n",
    "**Exercise:** Create a plot with two curves:  \n",
    "1. `scores` over episodes.  \n",
    "2. `epsilons` over episodes.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cbb0063",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,4))\n",
    "plt.plot(scores, label=\"Score per Episode\")\n",
    "plt.plot(epsilons, label=\"Epsilon (Exploration Rate)\")\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Value\")\n",
    "plt.title(\"Snake Game: Exploration vs Exploitation\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb17bb6f",
   "metadata": {},
   "source": [
    "<a name='6'></a>\n",
    "## 6 - Exercises  \n",
    "\n",
    "1. Modify the decay schedule: make $\\epsilon$ decrease **slower** or **faster**.  \n",
    "2. Try a **larger grid size** for the snake. What changes?  \n",
    "3. Fix $\\epsilon=1.0$ (always random). What happens to the score trend?  \n",
    "4. Fix $\\epsilon=0.0$ (always exploit). Why does the agent fail?  \n",
    "5. Add a new reward: -1 per move. Does the agent play differently?  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "849af7e4",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e242e6a7",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
