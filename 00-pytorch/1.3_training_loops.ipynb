{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7796a5ec",
   "metadata": {},
   "source": [
    "<div style=\"text-align:left;\">\n",
    "  <a href=\"https://code213.tech/\" target=\"_blank\">\n",
    "    <img src=\"code213.PNG\" alt=\"code213\">\n",
    "  </a>\n",
    "  <p><em>prepared by Latreche Sara</em></p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d137e32a",
   "metadata": {},
   "source": [
    "# Training Loops in PyTorch\n",
    "\n",
    "Training a neural network involves repeatedly performing a **forward pass**, computing a **loss**, performing **backpropagation**, and updating the model parameters.  \n",
    "\n",
    "PyTorch provides flexibility to write **custom training loops**, which is useful for:  \n",
    "- Debugging models  \n",
    "- Implementing custom optimizers or learning schedules  \n",
    "- Controlling how batches are processed  \n",
    "\n",
    "In this notebook, we will cover:  \n",
    "- Writing a simple training loop from scratch  \n",
    "- Using batches with **DataLoader**  \n",
    "- Updating weights using an **optimizer**  \n",
    "- Monitoring loss during training  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78cea97a",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "- [1 - Components of a Training Loop](#1)\n",
    "- [2 - Writing a Basic Training Loop](#2)\n",
    "- [3 - Using Optimizers](#3)\n",
    "- [4 - Using DataLoader for Mini-Batches](#4)\n",
    "- [5 - Monitoring Loss and Accuracy](#5)\n",
    "- [6 - Practice Exercises](#6)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59a1c126",
   "metadata": {},
   "source": [
    "<a name='1'></a>\n",
    "## 1 - Components of a Training Loop\n",
    "\n",
    "A typical PyTorch training loop has four main components:\n",
    "\n",
    "1. **Forward Pass**  \n",
    "   - Pass input data through the model to get predictions.  \n",
    "   - Example: `outputs = model(inputs)`\n",
    "\n",
    "2. **Loss Computation**  \n",
    "   - Compare predictions with the target labels using a loss function.  \n",
    "   - Example: `loss = loss_fn(outputs, labels)`\n",
    "\n",
    "3. **Backward Pass (Backpropagation)**  \n",
    "   - Compute gradients of the loss with respect to model parameters.  \n",
    "   - Example: `loss.backward()`\n",
    "\n",
    "4. **Optimizer Step (Parameter Update)**  \n",
    "   - Update model parameters based on computed gradients.  \n",
    "   - Example:  \n",
    "     ```python\n",
    "     optimizer.step()\n",
    "     optimizer.zero_grad()  # Reset gradients for next iteration\n",
    "     ```\n",
    "\n",
    "**Key Notes:**  \n",
    "- Always reset gradients (`optimizer.zero_grad()`) before the next forward pass.  \n",
    "- Loss and metrics can be logged to monitor training progress.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4ea41ae",
   "metadata": {},
   "source": [
    "<a name='2'></a>\n",
    "## 2 - Writing a Basic Training Loop\n",
    "\n",
    "Here we demonstrate a minimal training loop in PyTorch using a simple **linear model** and **MSE loss**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bdef605b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [20/100], Loss: 0.0205\n",
      "Epoch [40/100], Loss: 0.0074\n",
      "Epoch [60/100], Loss: 0.0066\n",
      "Epoch [80/100], Loss: 0.0058\n",
      "Epoch [100/100], Loss: 0.0052\n",
      "Predicted output after training: tensor([[2.8843],\n",
      "        [4.9439],\n",
      "        [7.0036],\n",
      "        [9.0632]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Sample dataset: y = 2x + 1\n",
    "x_train = torch.tensor([[1.0], [2.0], [3.0], [4.0]])\n",
    "y_train = torch.tensor([[3.0], [5.0], [7.0], [9.0]])\n",
    "\n",
    "# Define a simple linear model\n",
    "model = nn.Linear(1, 1)\n",
    "\n",
    "# Loss function\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "# Optimizer\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "# Training loop for 100 epochs\n",
    "for epoch in range(100):\n",
    "    # Forward pass\n",
    "    outputs = model(x_train)\n",
    "    loss = loss_fn(outputs, y_train)\n",
    "    \n",
    "    # Backward pass\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    \n",
    "    # Update parameters\n",
    "    optimizer.step()\n",
    "    \n",
    "    if (epoch+1) % 20 == 0:\n",
    "        print(f\"Epoch [{epoch+1}/100], Loss: {loss.item():.4f}\")\n",
    "\n",
    "# Test the model\n",
    "print(\"Predicted output after training:\", model(x_train).detach())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ece71cb",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1c1e1751",
   "metadata": {},
   "source": [
    "<a name='3'></a>\n",
    "## 3 - Using Optimizers\n",
    "\n",
    "Optimizers are responsible for **updating model parameters** based on computed gradients.  \n",
    "PyTorch provides many built-in optimizers in `torch.optim`.\n",
    "\n",
    "\n",
    "\n",
    "### 1. Stochastic Gradient Descent (SGD)\n",
    "\n",
    "- Update rule:  \n",
    "\n",
    "$$\n",
    "\\theta = \\theta - \\eta \\cdot \\nabla_\\theta L\n",
    "$$  \n",
    "\n",
    "where:  \n",
    "- $\\theta$ = model parameters  \n",
    "- $\\eta$ = learning rate  \n",
    "- $\\nabla_\\theta L$ = gradient of the loss w.r.t parameters  \n",
    "\n",
    "\n",
    "\n",
    "### 2. SGD with Momentum\n",
    "\n",
    "- Update rule with momentum:\n",
    "\n",
    "$$\n",
    "v_t = \\gamma v_{t-1} + \\eta \\nabla_\\theta L \\\\\n",
    "\\theta = \\theta - v_t\n",
    "$$  \n",
    "\n",
    "where $\\gamma$ is the momentum factor.\n",
    "\n",
    "\n",
    "\n",
    "### 3. Adam (Adaptive Moment Estimation)\n",
    "\n",
    "- Combines momentum and adaptive learning rates:\n",
    "\n",
    "$$\n",
    "m_t = \\beta_1 m_{t-1} + (1-\\beta_1) \\nabla_\\theta L \\\\\n",
    "v_t = \\beta_2 v_{t-1} + (1-\\beta_2) (\\nabla_\\theta L)^2 \\\\\n",
    "\\hat{\\theta} = \\theta - \\eta \\frac{m_t / (1-\\beta_1^t)}{\\sqrt{v_t / (1-\\beta_2^t)} + \\epsilon}\n",
    "$$\n",
    "\n",
    "\n",
    "### Key Points\n",
    "\n",
    "- Always pass `model.parameters()` to the optimizer.  \n",
    "- Call `optimizer.zero_grad()` **before each backward pass** to avoid gradient accumulation.  \n",
    "- Choose optimizer and learning rate carefully depending on the problem.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccc120f1",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "39f37830",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Example model\n",
    "model = nn.Linear(2, 1)\n",
    "\n",
    "# 1. SGD\n",
    "optimizer_sgd = optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "# 2. SGD with Momentum\n",
    "optimizer_momentum = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "\n",
    "# 3. Adam\n",
    "optimizer_adam = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98d38153",
   "metadata": {},
   "source": [
    "<a name='4'></a>\n",
    "## 4 - Using DataLoader for Mini-Batches\n",
    "\n",
    "Training on the **entire dataset at once** can be inefficient, especially for large datasets.  \n",
    "Instead, we use **mini-batches** to:  \n",
    "- Reduce memory usage  \n",
    "- Stabilize training with gradient estimates  \n",
    "- Increase convergence speed  \n",
    "\n",
    "PyTorch provides **`torch.utils.data.DataLoader`** to handle batching and shuffling.\n",
    "\n",
    "\n",
    "\n",
    "### Key Concepts\n",
    "\n",
    "1. **Dataset**\n",
    "- A PyTorch `Dataset` stores samples and their corresponding labels.  \n",
    "- Must implement `__len__()` and `__getitem__()`.\n",
    "\n",
    "2. **DataLoader**\n",
    "- Wraps a `Dataset` to create an iterable over **mini-batches**.  \n",
    "- Key parameters:\n",
    "  - `batch_size`: number of samples per batch  \n",
    "  - `shuffle`: whether to shuffle data each epoch  \n",
    "\n",
    "\n",
    "\n",
    "**Training Loop with Mini-Batches**\n",
    "- Iterate over the DataLoader instead of the full dataset.\n",
    "- Forward pass, compute loss, backward pass, and optimizer step for each batch.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "350be0a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1\n",
      "x: tensor([[-0.7705, -1.1865],\n",
      "        [-1.6320, -0.7430],\n",
      "        [-0.4538, -1.1619]])\n",
      "y: tensor([[1.],\n",
      "        [1.],\n",
      "        [0.]])\n",
      "---\n",
      "Batch 2\n",
      "x: tensor([[-0.2831, -0.3035],\n",
      "        [-0.4125, -0.2890],\n",
      "        [-0.1959, -0.8946]])\n",
      "y: tensor([[1.],\n",
      "        [0.],\n",
      "        [0.]])\n",
      "---\n",
      "Batch 3\n",
      "x: tensor([[-1.3389, -0.3129],\n",
      "        [ 0.7052,  1.3549],\n",
      "        [ 0.6676, -0.0868]])\n",
      "y: tensor([[1.],\n",
      "        [0.],\n",
      "        [0.]])\n",
      "---\n",
      "Batch 4\n",
      "x: tensor([[-0.9322, -1.4997]])\n",
      "y: tensor([[1.]])\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Sample dataset\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self):\n",
    "        self.x = torch.randn(10, 2)\n",
    "        self.y = torch.randint(0, 2, (10, 1)).float()\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.x[idx], self.y[idx]\n",
    "\n",
    "dataset = MyDataset()\n",
    "dataloader = DataLoader(dataset, batch_size=3, shuffle=True)\n",
    "\n",
    "# Example: iterate through mini-batches\n",
    "for batch_idx, (x_batch, y_batch) in enumerate(dataloader):\n",
    "    print(f\"Batch {batch_idx+1}\")\n",
    "    print(\"x:\", x_batch)\n",
    "    print(\"y:\", y_batch)\n",
    "    print(\"---\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceb21a7c",
   "metadata": {},
   "source": [
    "<a name='5'></a>\n",
    "## 5 - Monitoring Loss and Accuracy\n",
    "\n",
    "Monitoring **loss** and **accuracy** during training is essential to:  \n",
    "- Check if the model is learning  \n",
    "- Detect overfitting or underfitting  \n",
    "- Tune hyperparameters  \n",
    "\n",
    "\n",
    "\n",
    "### Key Points\n",
    "\n",
    "1. **Loss Tracking**  \n",
    "- Compute loss for each batch or epoch.  \n",
    "- Plot or print loss to observe convergence.\n",
    "\n",
    "2. **Accuracy Tracking (for classification)**  \n",
    "- Compare predicted labels with true labels:  \n",
    "\n",
    "$$\n",
    "\\text{Accuracy} = \\frac{\\text{Number of correct predictions}}{\\text{Total predictions}}\n",
    "$$\n",
    "\n",
    "- Can be computed per batch or per epoch.  \n",
    "\n",
    "\n",
    "\n",
    "**Tips**\n",
    "- Accumulate loss over all batches to compute **epoch loss**.  \n",
    "- For accuracy, round or take `argmax` of model outputs for class predictions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ba540f66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Loss=0.6485, Accuracy=0.60\n",
      "Epoch 2: Loss=0.6474, Accuracy=0.60\n",
      "Epoch 3: Loss=0.6429, Accuracy=0.70\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Sample data: 10 samples, 2 features, 2 classes\n",
    "x = torch.randn(10, 2)\n",
    "y = torch.randint(0, 2, (10,))\n",
    "\n",
    "dataset = TensorDataset(x, y)\n",
    "dataloader = DataLoader(dataset, batch_size=3, shuffle=True)\n",
    "\n",
    "# Simple model\n",
    "model = nn.Linear(2, 2)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "# Training loop with loss and accuracy tracking\n",
    "for epoch in range(3):  # small number of epochs for demo\n",
    "    epoch_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for x_batch, y_batch in dataloader:\n",
    "        # Forward\n",
    "        outputs = model(x_batch)\n",
    "        loss = loss_fn(outputs, y_batch)\n",
    "        \n",
    "        # Backward\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Track loss\n",
    "        epoch_loss += loss.item() * x_batch.size(0)\n",
    "        \n",
    "        # Track accuracy\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        correct += (predicted == y_batch).sum().item()\n",
    "        total += y_batch.size(0)\n",
    "    \n",
    "    epoch_loss /= total\n",
    "    accuracy = correct / total\n",
    "    print(f\"Epoch {epoch+1}: Loss={epoch_loss:.4f}, Accuracy={accuracy:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6188f67b",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3aa60a6e",
   "metadata": {},
   "source": [
    "<a name='6'></a>\n",
    "## 6 - Practice Exercises\n",
    "\n",
    "Try the following exercises to reinforce your understanding of **training loops, optimizers, and mini-batches**.\n",
    "\n",
    "\n",
    "\n",
    "### **Exercise 1: Forward and Backward Pass**\n",
    "- Create a simple linear model with input size 3 â†’ output size 1.  \n",
    "- Generate random input data (`batch_size=4`) and targets.  \n",
    "- Perform a forward pass, compute **MSE loss**, and run a backward pass.\n",
    "\n",
    "\n",
    "\n",
    "### **Exercise 2: Optimizer Step**\n",
    "- Use **SGD** with learning rate 0.01.  \n",
    "- Update the model parameters using `optimizer.step()`.  \n",
    "- Print the **loss before and after the optimizer step**.\n",
    "\n",
    "\n",
    "### **Exercise 3: Mini-Batch Training**\n",
    "- Create a dataset of 12 samples, input size 2, output size 1.  \n",
    "- Use `DataLoader` with `batch_size=4`.  \n",
    "- Train the model for **2 epochs**, computing and printing loss for each batch.\n",
    "\n",
    "\n",
    "\n",
    "### **Exercise 4: Accuracy Tracking (Optional Challenge)**\n",
    "- Modify the dataset to have **2 classes**.  \n",
    "- Use **CrossEntropyLoss** and a linear model with 2 output neurons.  \n",
    "- Compute and print **accuracy per epoch** over 2 epochs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d3188bd",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a70a01fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exercise 1 - Loss: 3.2175486087799072\n",
      "Exercise 2 - Loss after optimizer step: 3.009896755218506\n",
      "Epoch 1, Batch 1, Loss: 0.8909\n",
      "Epoch 1, Batch 2, Loss: 1.7080\n",
      "Epoch 1, Batch 3, Loss: 0.6876\n",
      "Epoch 2, Batch 1, Loss: 1.4694\n",
      "Epoch 2, Batch 2, Loss: 1.0620\n",
      "Epoch 2, Batch 3, Loss: 0.5712\n",
      "Epoch 1, Accuracy: 0.62\n",
      "Epoch 2, Accuracy: 0.62\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "import torch.optim as optim\n",
    "\n",
    "# ----------------------------\n",
    "# Exercise 1: Forward and Backward Pass\n",
    "# ----------------------------\n",
    "model = nn.Linear(3, 1)\n",
    "x = torch.randn(4, 3)\n",
    "y = torch.randn(4, 1)\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "# Forward\n",
    "outputs = model(x)\n",
    "loss = loss_fn(outputs, y)\n",
    "print(\"Exercise 1 - Loss:\", loss.item())\n",
    "\n",
    "# Backward\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "optimizer.zero_grad()\n",
    "loss.backward()\n",
    "optimizer.step()\n",
    "\n",
    "# ----------------------------\n",
    "# Exercise 2: Optimizer Step\n",
    "# ----------------------------\n",
    "outputs_after = model(x)\n",
    "loss_after = loss_fn(outputs_after, y)\n",
    "print(\"Exercise 2 - Loss after optimizer step:\", loss_after.item())\n",
    "\n",
    "# ----------------------------\n",
    "# Exercise 3: Mini-Batch Training\n",
    "# ----------------------------\n",
    "x_data = torch.randn(12, 2)\n",
    "y_data = torch.randn(12, 1)\n",
    "dataset = TensorDataset(x_data, y_data)\n",
    "dataloader = DataLoader(dataset, batch_size=4, shuffle=True)\n",
    "\n",
    "model = nn.Linear(2, 1)\n",
    "loss_fn = nn.MSELoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "for epoch in range(2):\n",
    "    for batch_idx, (x_batch, y_batch) in enumerate(dataloader):\n",
    "        outputs = model(x_batch)\n",
    "        loss = loss_fn(outputs, y_batch)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}, Batch {batch_idx+1}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "# ----------------------------\n",
    "# Exercise 4: Accuracy Tracking (Optional)\n",
    "# ----------------------------\n",
    "# Dataset with 2 classes\n",
    "x_cls = torch.randn(8, 2)\n",
    "y_cls = torch.randint(0, 2, (8,))\n",
    "dataset_cls = TensorDataset(x_cls, y_cls)\n",
    "dataloader_cls = DataLoader(dataset_cls, batch_size=4, shuffle=True)\n",
    "\n",
    "model_cls = nn.Linear(2, 2)  # 2 output neurons\n",
    "loss_fn_cls = nn.CrossEntropyLoss()\n",
    "optimizer_cls = optim.SGD(model_cls.parameters(), lr=0.01)\n",
    "\n",
    "for epoch in range(2):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for x_batch, y_batch in dataloader_cls:\n",
    "        outputs = model_cls(x_batch)\n",
    "        loss = loss_fn_cls(outputs, y_batch)\n",
    "        \n",
    "        optimizer_cls.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer_cls.step()\n",
    "        \n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        correct += (predicted == y_batch).sum().item()\n",
    "        total += y_batch.size(0)\n",
    "    \n",
    "    accuracy = correct / total\n",
    "    print(f\"Epoch {epoch+1}, Accuracy: {accuracy:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2b8d4a9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
