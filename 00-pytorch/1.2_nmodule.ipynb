{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e3b9f85b",
   "metadata": {},
   "source": [
    "<div style=\"text-align:left;\">\n",
    "  <a href=\"https://code213.tech/\" target=\"_blank\">\n",
    "    <img src=\"../code213.PNG\" alt=\"code213\">\n",
    "  </a>\n",
    "  <p><em>prepared by Latreche Sara</em></p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1d1d968",
   "metadata": {},
   "source": [
    "# Neural Network Modules  \n",
    "\n",
    "PyTorch provides the **`torch.nn`** module, which contains:  \n",
    "- **Predefined layers** (linear, convolution, dropout, etc.).  \n",
    "- **Activation functions** (ReLU, Sigmoid, Tanh, etc.).  \n",
    "- **Loss functions** (MSE, CrossEntropy, etc.).  \n",
    "- **Model containers** (e.g., `nn.Sequential`, custom `nn.Module` classes).  \n",
    "\n",
    "The `torch.nn.Module` class is the **base class** for all neural network models in PyTorch.  \n",
    "Every custom neural network you build will extend this class and implement the `forward()` method.  \n",
    "\n",
    "This notebook will introduce how to use **nn.Module** to build neural networks step by step.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29c7f088",
   "metadata": {},
   "source": [
    "## Table of Contents  \n",
    "\n",
    "- [1 - Introduction to `nn.Module`](#1)  \n",
    "- [2 - Linear Layers](#2)  \n",
    "- [3 - Activation Functions](#3)  \n",
    "- [4 - Building a Model with `nn.Sequential`](#4)  \n",
    "- [5 - Custom Neural Networks with `nn.Module`](#5)  \n",
    "- [6 - Loss Functions](#6)  \n",
    "- [7 - Practice Exercises](#7)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6173b7cc",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0b0d0690",
   "metadata": {},
   "source": [
    "<a name='1'></a>\n",
    "## 1 - Introduction to `nn.Module`  \n",
    "\n",
    "In PyTorch, all neural networks are built using the base class **`torch.nn.Module`**.  \n",
    "- A **module** can be a single layer (e.g., `nn.Linear`) or a complete model.  \n",
    "- When you create your own network, you subclass `nn.Module` and define the layers inside `__init__()`.  \n",
    "- The forward pass of the network is defined inside the `forward()` method. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c43cb8fb",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1f6a7221",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model output:\n",
      " tensor([[-0.0127],\n",
      "        [-0.0882],\n",
      "        [-0.1637]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class SimpleModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleModel, self).__init__()\n",
    "        self.linear = nn.Linear(2, 1)  # Linear layer with 2 inputs and 1 output\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "\n",
    "# Create an instance\n",
    "model = SimpleModel()\n",
    "\n",
    "# Input: batch of 3 samples, each with 2 features\n",
    "x = torch.tensor([[1.0, 2.0],\n",
    "                  [2.0, 3.0],\n",
    "                  [3.0, 4.0]])\n",
    "\n",
    "# Forward pass\n",
    "output = model(x)\n",
    "print(\"Model output:\\n\", output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ac99eb3",
   "metadata": {},
   "source": [
    "<a name='2'></a>\n",
    "## 2 - Linear Layers  \n",
    "\n",
    "The **linear layer** (also called a fully connected or dense layer) is one of the most fundamental building blocks in neural networks.  \n",
    "\n",
    "In PyTorch, it is implemented as:  \n",
    "\n",
    "```python\n",
    "nn.Linear(in_features, out_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0e39877f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:\n",
      " tensor([[-0.4059,  0.2126,  0.0979],\n",
      "        [-0.2269, -0.1264, -0.8216],\n",
      "        [ 1.0402, -0.0248,  1.8817],\n",
      "        [-0.7757,  0.3093, -0.5460]])\n",
      "Output:\n",
      " tensor([[-0.4505, -0.2330],\n",
      "        [-0.5220,  0.1017],\n",
      "        [-0.2077, -0.8706],\n",
      "        [-0.4882,  0.0284]], grad_fn=<AddmmBackward0>)\n",
      "Weights:\n",
      " Parameter containing:\n",
      "tensor([[ 0.2937,  0.4725, -0.0392],\n",
      "        [ 0.1800,  0.3476, -0.4571]], requires_grad=True)\n",
      "Bias:\n",
      " Parameter containing:\n",
      "tensor([-0.4279, -0.1891], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "# Linear layer with 3 input features → 2 output features\n",
    "linear = nn.Linear(3, 2)\n",
    "\n",
    "# Random input: batch of 4 samples, each with 3 features\n",
    "x = torch.randn(4, 3)\n",
    "\n",
    "# Forward pass\n",
    "y = linear(x)\n",
    "\n",
    "print(\"Input:\\n\", x)\n",
    "print(\"Output:\\n\", y)\n",
    "print(\"Weights:\\n\", linear.weight)\n",
    "print(\"Bias:\\n\", linear.bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e23f0715",
   "metadata": {},
   "source": [
    "<a name='3'></a>\n",
    "## 3 - Activation Functions  \n",
    "\n",
    "Activation functions introduce **non-linearity** into neural networks, which allows them to model complex relationships.  \n",
    "In PyTorch, many activations are available in `torch.nn`.  \n",
    "\n",
    "### Common Activation Functions  \n",
    "\n",
    "1. **ReLU (Rectified Linear Unit)**  \n",
    "$$\n",
    "f(x) = \\max(0, x)\n",
    "$$  \n",
    "\n",
    "2. **Sigmoid**  \n",
    "$$\n",
    "f(x) = \\frac{1}{1 + e^{-x}}\n",
    "$$  \n",
    "\n",
    "3. **Tanh (Hyperbolic Tangent)**  \n",
    "$$\n",
    "f(x) = \\tanh(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}}\n",
    "$$  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "290cbdfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: tensor([[-1.,  0.,  1.,  2.]])\n",
      "ReLU: tensor([[0., 0., 1., 2.]])\n",
      "Sigmoid: tensor([[0.2689, 0.5000, 0.7311, 0.8808]])\n",
      "Tanh: tensor([[-0.7616,  0.0000,  0.7616,  0.9640]])\n"
     ]
    }
   ],
   "source": [
    "# Sample input\n",
    "x = torch.tensor([[-1.0, 0.0, 1.0, 2.0]])\n",
    "\n",
    "# Define activations\n",
    "relu = nn.ReLU()\n",
    "sigmoid = nn.Sigmoid()\n",
    "tanh = nn.Tanh()\n",
    "\n",
    "print(\"Input:\", x)\n",
    "print(\"ReLU:\", relu(x))\n",
    "print(\"Sigmoid:\", sigmoid(x))\n",
    "print(\"Tanh:\", tanh(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fd9ec98",
   "metadata": {},
   "source": [
    "<a name='4'></a>\n",
    "## 4 - Building a Model with `nn.Sequential`\n",
    "\n",
    "In PyTorch, the **`nn.Sequential`** container allows us to build a model by stacking layers in order.  \n",
    "This is useful for **simple feed-forward neural networks**.\n",
    "\n",
    "---\n",
    "\n",
    "### Example: A Small Neural Network\n",
    "\n",
    "Suppose we want a model:\n",
    "\n",
    "- Input size = 4  \n",
    "- Hidden layer with 8 neurons (ReLU activation)  \n",
    "- Output size = 2 (e.g., binary classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b474533b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Linear(in_features=4, out_features=8, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=8, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "# Define the model\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(4, 8),   # Layer 1: Linear transformation (4 → 8)\n",
    "    nn.ReLU(),         # Activation function\n",
    "    nn.Linear(8, 2)    # Layer 2: Linear transformation (8 → 2)\n",
    ")\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5e519851",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: tensor([[-0.6535,  1.1033, -0.4188,  0.6314],\n",
      "        [-0.1038, -0.0235, -0.6025,  1.2412],\n",
      "        [-0.5209,  1.6439, -0.9290,  1.0729]])\n",
      "Output: tensor([[0.3381, 0.5965],\n",
      "        [0.3853, 0.5629],\n",
      "        [0.4075, 0.7071]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "#Example: Forward Pass\n",
    "\n",
    "# Sample input: batch of 3 samples, each with 4 features\n",
    "x = torch.randn(3, 4)\n",
    "\n",
    "# Pass through the model\n",
    "output = model(x)\n",
    "\n",
    "print(\"Input:\", x)\n",
    "print(\"Output:\", output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93e56eec",
   "metadata": {},
   "source": [
    "<a name='5'></a>\n",
    "## 5 - Defining Custom Models with `nn.Module`\n",
    "\n",
    "While `nn.Sequential` is convenient for simple networks, it becomes **limiting** when we need:  \n",
    "- Multiple inputs/outputs  \n",
    "- Custom operations inside the forward pass  \n",
    "- More complex architectures  \n",
    "\n",
    "In such cases, we subclass **`nn.Module`**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "94f04fd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MyModel(\n",
      "  (fc1): Linear(in_features=4, out_features=8, bias=True)\n",
      "  (fc2): Linear(in_features=8, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "# Define a custom model by subclassing nn.Module\n",
    "class MyModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(4, 8)   # First layer\n",
    "        self.fc2 = nn.Linear(8, 2)   # Second layer\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Define forward pass\n",
    "        x = F.relu(self.fc1(x))   # Apply ReLU after first layer\n",
    "        x = self.fc2(x)           # Output layer\n",
    "        return x\n",
    "\n",
    "# Instantiate the model\n",
    "model = MyModel()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "807b0ef1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: tensor([[-0.2067,  0.6353,  0.7016,  0.5031],\n",
      "        [ 0.7267, -0.0069,  2.0131, -0.2274],\n",
      "        [-0.9513, -1.0209, -0.0299, -0.2787]])\n",
      "Output: tensor([[ 0.0930, -0.3943],\n",
      "        [ 0.0132, -0.5313],\n",
      "        [ 0.3193, -0.2644]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "#Forward Pass Example\n",
    "x = torch.randn(3, 4)  # 3 samples, 4 features each\n",
    "output = model(x)\n",
    "\n",
    "print(\"Input:\", x)\n",
    "print(\"Output:\", output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03fc29c7",
   "metadata": {},
   "source": [
    "<a name='6'></a>\n",
    "## 6 - Loss Functions\n",
    "\n",
    "Loss functions measure how well the model's predictions match the target labels.  \n",
    "In PyTorch, loss functions are available in **`torch.nn`**.\n",
    "\n",
    "---\n",
    "\n",
    "### Common Loss Functions\n",
    "\n",
    "1. **Mean Squared Error (MSE)** – for regression:\n",
    "\n",
    "$$\n",
    "\\text{MSE} = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c749a393",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE Loss: 0.1666666716337204\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "# Sample predictions and targets\n",
    "y_pred = torch.tensor([2.0, 3.0, 4.0])\n",
    "y_true = torch.tensor([1.5, 2.5, 4.0])\n",
    "\n",
    "# Define MSE loss\n",
    "mse_loss = nn.MSELoss()\n",
    "loss = mse_loss(y_pred, y_true)\n",
    "print(\"MSE Loss:\", loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c5e1b319",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-Entropy Loss: 0.32173651456832886\n"
     ]
    }
   ],
   "source": [
    "# Sample logits (not probabilities) for 3 classes and batch of 2\n",
    "logits = torch.tensor([[2.0, 0.5, 1.0],\n",
    "                       [1.0, 3.0, 0.2]])\n",
    "labels = torch.tensor([0, 1])  # Target class indices\n",
    "\n",
    "# Define cross-entropy loss\n",
    "ce_loss = nn.CrossEntropyLoss()\n",
    "loss = ce_loss(logits, labels)\n",
    "print(\"Cross-Entropy Loss:\", loss.item())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "474098cb",
   "metadata": {},
   "source": [
    "<a name='7'></a>\n",
    "## 7 - Practice Exercises\n",
    "\n",
    "Try the following exercises to reinforce your understanding of **nn.Module, layers, and loss functions**.\n",
    "\n",
    "\n",
    "\n",
    "### **Exercise 1: Build a Simple Model**\n",
    "- Create a custom model using `nn.Module` with:  \n",
    "  - Input size = 3  \n",
    "  - Hidden layer = 5 neurons, ReLU activation  \n",
    "  - Output layer = 2 neurons  \n",
    "- Print the model summary.  \n",
    "\n",
    "\n",
    "\n",
    "### **Exercise 2: Forward Pass**\n",
    "- Create a random input tensor with shape `[4, 3]` (batch of 4 samples, 3 features).  \n",
    "- Pass it through your model and print the output.  \n",
    "\n",
    "\n",
    "\n",
    "### **Exercise 3: Compute Loss**\n",
    "- Assume the target labels for your batch are `[0, 1, 1, 0]`.  \n",
    "- Use `nn.CrossEntropyLoss()` to compute the loss between model output and target labels.  \n",
    "\n",
    "\n",
    "\n",
    "### **Exercise 4 (Optional Challenge)**\n",
    "- Modify your model to add a second hidden layer (5 → 4 → 2).  \n",
    "- Use ReLU after each hidden layer.  \n",
    "- Perform a forward pass and compute the loss.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "df3aebe5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exercise 1 - Model Summary:\n",
      " SimpleModel(\n",
      "  (fc1): Linear(in_features=3, out_features=5, bias=True)\n",
      "  (fc2): Linear(in_features=5, out_features=2, bias=True)\n",
      ")\n",
      "\n",
      "Exercise 2 - Forward Pass Output:\n",
      " tensor([[ 0.1645, -0.0315],\n",
      "        [ 0.2513, -0.0101],\n",
      "        [ 0.2480,  0.0928],\n",
      "        [-0.5656, -0.8524]], grad_fn=<AddmmBackward0>)\n",
      "\n",
      "Exercise 3 - CrossEntropy Loss: 0.6915081143379211\n",
      "\n",
      "Exercise 4 - Extended Model Output:\n",
      " tensor([[ 0.1505, -0.0812],\n",
      "        [ 0.1052, -0.0647],\n",
      "        [ 0.1829, -0.0658],\n",
      "        [-0.0662,  0.0961]], grad_fn=<AddmmBackward0>)\n",
      "Exercise 4 - CrossEntropy Loss: 0.742132842540741\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# ----------------------------\n",
    "# Exercise 1: Build a Simple Model\n",
    "# ----------------------------\n",
    "class SimpleModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(3, 5)   # Input 3 → Hidden 5\n",
    "        self.fc2 = nn.Linear(5, 2)   # Hidden 5 → Output 2\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "model = SimpleModel()\n",
    "print(\"Exercise 1 - Model Summary:\\n\", model)\n",
    "\n",
    "# ----------------------------\n",
    "# Exercise 2: Forward Pass\n",
    "# ----------------------------\n",
    "x = torch.randn(4, 3)  # Batch of 4 samples, 3 features\n",
    "output = model(x)\n",
    "print(\"\\nExercise 2 - Forward Pass Output:\\n\", output)\n",
    "\n",
    "# ----------------------------\n",
    "# Exercise 3: Compute Loss\n",
    "# ----------------------------\n",
    "# Target labels for the batch\n",
    "labels = torch.tensor([0, 1, 1, 0])\n",
    "\n",
    "# Define CrossEntropyLoss\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "loss = loss_fn(output, labels)\n",
    "print(\"\\nExercise 3 - CrossEntropy Loss:\", loss.item())\n",
    "\n",
    "# ----------------------------\n",
    "# Exercise 4 (Optional Challenge)\n",
    "# ----------------------------\n",
    "class ExtendedModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ExtendedModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(3, 5)\n",
    "        self.fc2 = nn.Linear(5, 4)\n",
    "        self.fc3 = nn.Linear(4, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "extended_model = ExtendedModel()\n",
    "output_extended = extended_model(x)\n",
    "loss_extended = loss_fn(output_extended, labels)\n",
    "\n",
    "print(\"\\nExercise 4 - Extended Model Output:\\n\", output_extended)\n",
    "print(\"Exercise 4 - CrossEntropy Loss:\", loss_extended.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f032f26",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
