{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "54b11b38",
   "metadata": {},
   "source": [
    "<div style=\"text-align:left;\">\n",
    "  <a href=\"https://code213.tech/\" target=\"_blank\">\n",
    "    <img src=\"../code213.PNG\" alt=\"code213\">\n",
    "  </a>\n",
    "  <p><em>prepared by Latreche Sara</em></p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6183e813",
   "metadata": {},
   "source": [
    "# 10.6 - GPU Acceleration in PyTorch\n",
    "\n",
    "PyTorch allows you to **accelerate computations** by using GPUs instead of CPUs.  \n",
    "GPUs are specialized for **parallel computations**, which makes them ideal for deep learning.  \n",
    "\n",
    "### Key Points\n",
    "1. **CPU vs GPU**\n",
    "   - CPU: Handles sequential tasks efficiently  \n",
    "   - GPU: Handles many tasks in parallel → faster matrix and tensor operations  \n",
    "\n",
    "2. **Tensor Device**\n",
    "   - Tensors can be on CPU (`torch.device('cpu')`) or GPU (`torch.device('cuda')`)  \n",
    "   - Move tensors between devices using `.to(device)`  \n",
    "\n",
    "3. **Why GPU speeds up training**\n",
    "   - Neural network operations are **highly parallelizable**  \n",
    "   - GPU can handle thousands of operations simultaneously  \n",
    "\n",
    "4. **Optional: TPU**\n",
    "   - Tensor Processing Units (TPUs) are specialized accelerators for tensor operations, often used in Google Cloud\n",
    "\n",
    "---\n",
    "\n",
    "In this notebook, we will cover:  \n",
    "- Checking if GPU is available  \n",
    "- Moving tensors and models to GPU  \n",
    "- Performing operations on GPU  \n",
    "- Comparing CPU vs GPU speed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "948f8779",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "- [1 - Checking Device](#1)\n",
    "- [2 - Moving Tensors to GPU](#2)\n",
    "- [3 - Moving Models to GPU](#3)\n",
    "- [4 - Operations on GPU](#4)\n",
    "- [5 - Comparing CPU vs GPU](#5)\n",
    "- [6 - Practice Exercises](#6)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91538a73",
   "metadata": {},
   "source": [
    "<a name='1'></a>\n",
    "## 1 - Checking Device\n",
    "\n",
    "Before using GPU, we need to check if a CUDA-enabled GPU is available.  \n",
    "\n",
    "### Key Points\n",
    "- `torch.cuda.is_available()` returns `True` if a GPU is available  \n",
    "- `torch.device('cuda')` represents a GPU device  \n",
    "- `torch.device('cpu')` represents the CPU  \n",
    "\n",
    "This allows you to **write device-agnostic code**, which works on both CPU and GPU.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6c63ce18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU available: False\n",
      "Using device: cpu\n",
      "Tensor: tensor([1., 2., 3.])\n",
      "Tensor device: cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Check if GPU is available\n",
    "gpu_available = torch.cuda.is_available()\n",
    "print(\"GPU available:\", gpu_available)\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if gpu_available else 'cpu')\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# Create tensor on the chosen device\n",
    "x = torch.tensor([1.0, 2.0, 3.0], device=device)\n",
    "print(\"Tensor:\", x)\n",
    "print(\"Tensor device:\", x.device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d9ade7e",
   "metadata": {},
   "source": [
    "<a name='2'></a>\n",
    "## 2 - Moving Tensors to GPU\n",
    "\n",
    "You can move tensors between CPU and GPU using the `.to()` method or `.cuda()` / `.cpu()`.\n",
    "\n",
    "### Key Points\n",
    "- `tensor.to(device)` moves a tensor to the specified device  \n",
    "- `tensor.cuda()` moves a tensor to GPU  \n",
    "- `tensor.cpu()` moves a tensor back to CPU  \n",
    "\n",
    "This is useful for ensuring that **all tensors and models are on the same device** to avoid errors during computation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8d24701",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9c650f8d",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "422f4ae1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original tensor device: cpu\n",
      "Tensor on GPU device: cpu\n",
      "Tensor back on CPU: cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Create tensor on CPU\n",
    "x_cpu = torch.randn(3, 3)\n",
    "print(\"Original tensor device:\", x_cpu.device)\n",
    "\n",
    "# Move tensor to GPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "x_gpu = x_cpu.to(device)\n",
    "print(\"Tensor on GPU device:\", x_gpu.device)\n",
    "\n",
    "# Move tensor back to CPU\n",
    "x_back_cpu = x_gpu.cpu()\n",
    "print(\"Tensor back on CPU:\", x_back_cpu.device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fac26441",
   "metadata": {},
   "source": [
    "<a name='3'></a>\n",
    "## 3 - Moving Models to GPU\n",
    "\n",
    "To leverage GPU acceleration, both **tensors** and **models** must be on the same device.  \n",
    "\n",
    "### Key Points\n",
    "- Move a model to GPU using `model.to(device)`  \n",
    "- Ensure all inputs are also on the same device  \n",
    "- Forward and backward passes will then automatically use the GPU  \n",
    "\n",
    "This allows you to train models faster and handle larger datasets efficiently.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b41778d8",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5cd930ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original model device: cpu\n",
      "Model moved to device: cpu\n",
      "Output on device: cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Define a simple model\n",
    "model = nn.Linear(3, 1)\n",
    "print(\"Original model device:\", next(model.parameters()).device)\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Move model to GPU\n",
    "model = model.to(device)\n",
    "print(\"Model moved to device:\", next(model.parameters()).device)\n",
    "\n",
    "# Create input tensor on the same device\n",
    "x = torch.randn(2, 3).to(device)\n",
    "\n",
    "# Forward pass on GPU\n",
    "output = model(x)\n",
    "print(\"Output on device:\", output.device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "041a34ec",
   "metadata": {},
   "source": [
    "<a name='4'></a>\n",
    "## 4 - Operations on GPU\n",
    "\n",
    "Once tensors and models are on GPU, all operations are **performed on the GPU**.  \n",
    "\n",
    "### Key Points\n",
    "- PyTorch automatically uses the GPU for tensor operations if tensors are on `cuda` device  \n",
    "- You can perform standard operations: addition, multiplication, matrix multiplication, etc.  \n",
    "- Always ensure **all involved tensors are on the same device** to avoid runtime errors  \n",
    "\n",
    "Example operations:\n",
    "- Element-wise addition: `x + y`  \n",
    "- Matrix multiplication: `torch.matmul(a, b)`  \n",
    "- Activation functions: `torch.relu(tensor)`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6598b50c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Element-wise addition result device: cpu\n",
      "Matrix multiplication result device: cpu\n",
      "ReLU result device: cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Create tensors on GPU\n",
    "a = torch.randn(3, 3, device=device)\n",
    "b = torch.randn(3, 3, device=device)\n",
    "\n",
    "# Element-wise addition\n",
    "c = a + b\n",
    "print(\"Element-wise addition result device:\", c.device)\n",
    "\n",
    "# Matrix multiplication\n",
    "d = torch.matmul(a, b)\n",
    "print(\"Matrix multiplication result device:\", d.device)\n",
    "\n",
    "# Applying activation function\n",
    "e = torch.relu(d)\n",
    "print(\"ReLU result device:\", e.device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7e507b0",
   "metadata": {},
   "source": [
    "<a name='5'></a>\n",
    "## 5 - Comparing CPU vs GPU\n",
    "\n",
    "It is often useful to **measure the speedup** gained by using a GPU.  \n",
    "\n",
    "### Key Points\n",
    "- GPU accelerates **large tensor operations** much more than CPU  \n",
    "- Small operations may not show significant speedup due to overhead  \n",
    "- Use `torch.cuda.synchronize()` to accurately measure GPU time  \n",
    "\n",
    "Example operations to compare:\n",
    "- Matrix multiplication of large tensors  \n",
    "- Forward pass of a simple model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "34a470e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU time: 41.122729539871216 seconds\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import time\n",
    "\n",
    "# Set device\n",
    "device_cpu = torch.device('cpu')\n",
    "device_gpu = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Large tensors\n",
    "size = 10000\n",
    "x_cpu = torch.randn(size, size, device=device_cpu)\n",
    "y_cpu = torch.randn(size, size, device=device_cpu)\n",
    "\n",
    "# CPU timing\n",
    "start_cpu = time.time()\n",
    "z_cpu = torch.matmul(x_cpu, y_cpu)\n",
    "end_cpu = time.time()\n",
    "print(\"CPU time:\", end_cpu - start_cpu, \"seconds\")\n",
    "\n",
    "if device_gpu.type == 'cuda':\n",
    "    # Move tensors to GPU\n",
    "    x_gpu = x_cpu.to(device_gpu)\n",
    "    y_gpu = y_cpu.to(device_gpu)\n",
    "    \n",
    "    # GPU timing\n",
    "    torch.cuda.synchronize()  # Wait for all operations to finish\n",
    "    start_gpu = time.time()\n",
    "    z_gpu = torch.matmul(x_gpu, y_gpu)\n",
    "    torch.cuda.synchronize()\n",
    "    end_gpu = time.time()\n",
    "    print(\"GPU time:\", end_gpu - start_gpu, \"seconds\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6edab07",
   "metadata": {},
   "source": [
    "<a name='6'></a>\n",
    "## 6 - Practice Exercises\n",
    "\n",
    "Try the following exercises to reinforce your understanding of **GPU usage in PyTorch**:\n",
    "\n",
    "\n",
    "\n",
    "### **Exercise 1: Check GPU**\n",
    "- Write a script to check if a CUDA-enabled GPU is available.  \n",
    "- Set the device accordingly.\n",
    "\n",
    "\n",
    "\n",
    "### **Exercise 2: Move Tensors**\n",
    "- Create a tensor on CPU and move it to GPU.  \n",
    "- Perform a matrix multiplication on the GPU.  \n",
    "- Move the result back to CPU.\n",
    "\n",
    "\n",
    "\n",
    "### **Exercise 3: Move Model**\n",
    "- Create a simple linear model (input size 4 → output size 2).  \n",
    "- Move the model to GPU.  \n",
    "- Create a sample input tensor and perform a forward pass.\n",
    "\n",
    "\n",
    "\n",
    "### **Exercise 4: Timing CPU vs GPU**\n",
    "- Create two large random tensors (size 5000 x 5000).  \n",
    "- Compute the matrix multiplication on CPU and GPU.  \n",
    "- Measure and compare the time.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f0397a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import time\n",
    "\n",
    "# ----------------------------\n",
    "# Exercise 1: Check GPU\n",
    "# ----------------------------\n",
    "gpu_available = torch.cuda.is_available()\n",
    "device = torch.device('cuda' if gpu_available else 'cpu')\n",
    "print(\"GPU available:\", gpu_available)\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# ----------------------------\n",
    "# Exercise 2: Move Tensors\n",
    "# ----------------------------\n",
    "x_cpu = torch.randn(3, 3)\n",
    "y_cpu = torch.randn(3, 3)\n",
    "x_gpu = x_cpu.to(device)\n",
    "y_gpu = y_cpu.to(device)\n",
    "z_gpu = torch.matmul(x_gpu, y_gpu)\n",
    "z_cpu = z_gpu.cpu()\n",
    "print(\"Result on CPU after GPU matmul:\", z_cpu)\n",
    "\n",
    "# ----------------------------\n",
    "# Exercise 3: Move Model\n",
    "# ----------------------------\n",
    "model = nn.Linear(4, 2)\n",
    "model = model.to(device)\n",
    "input_tensor = torch.randn(1, 4).to(device)\n",
    "output = model(input_tensor)\n",
    "print(\"Model output on device:\", output)\n",
    "\n",
    "# ----------------------------\n",
    "# Exercise 4: Timing CPU vs GPU\n",
    "# ----------------------------\n",
    "size = 5000\n",
    "x = torch.randn(size, size)\n",
    "y = torch.randn(size, size)\n",
    "\n",
    "# CPU timing\n",
    "start_cpu = time.time()\n",
    "z_cpu = torch.matmul(x, y)\n",
    "end_cpu = time.time()\n",
    "print(\"CPU time:\", end_cpu - start_cpu, \"seconds\")\n",
    "\n",
    "if gpu_available:\n",
    "    x_gpu = x.to(device)\n",
    "    y_gpu = y.to(device)\n",
    "    torch.cuda.synchronize()\n",
    "    start_gpu = time.time()\n",
    "    z_gpu = torch.matmul(x_gpu, y_gpu)\n",
    "    torch.cuda.synchronize()\n",
    "    end_gpu = time.time()\n",
    "    print(\"GPU time:\", end_gpu - start_gpu, \"seconds\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
