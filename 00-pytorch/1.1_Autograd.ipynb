{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c585d41f",
   "metadata": {},
   "source": [
    "<div style=\"text-align:left;\">\n",
    "  <a href=\"https://code213.tech/\" target=\"_blank\">\n",
    "    <img src=\"code213.PNG\" alt=\"code213\">\n",
    "  </a>\n",
    "  <p><em>prepared by Latreche Sara</em></p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6ca37f8",
   "metadata": {},
   "source": [
    "# Autograd in PyTorch  \n",
    "\n",
    "One of PyTorch’s most powerful features is **autograd**, its automatic differentiation engine.  \n",
    "\n",
    "Autograd enables PyTorch to:  \n",
    "- **Record operations** performed on tensors with `requires_grad=True`.  \n",
    "- **Build a computational graph dynamically**, so we don’t need to define it manually.  \n",
    "- **Compute gradients automatically** using the `.backward()` method.  \n",
    "- Provide gradients through the `.grad` attribute, which are essential for **backpropagation** in neural networks.  \n",
    "\n",
    "In this notebook, we will explore:  \n",
    "- How to enable gradients with `requires_grad`.  \n",
    "- How PyTorch builds the computational graph.  \n",
    "- How to compute gradients with `.backward()`.  \n",
    "- How gradients are stored and reset.  \n",
    "- Practical examples of autograd in action.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24acea1a",
   "metadata": {},
   "source": [
    "## Table of Contents  \n",
    "\n",
    "- [1 - Enabling Autograd](#1)  \n",
    "- [2 - Building a Computational Graph](#2)  \n",
    "- [3 - Computing Gradients with `.backward()`](#3)  \n",
    "- [4 - Accessing and Resetting Gradients](#4)  \n",
    "- [5 - Example: Backpropagation in Action](#5)  \n",
    "- [6 - Practice Exercises](#6)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ec90fd5",
   "metadata": {},
   "source": [
    "<a name='1'></a>\n",
    "## 1 - Enabling Autograd  \n",
    "\n",
    "<a name='1'></a>\n",
    "## 1 - Enabling Autograd  \n",
    "\n",
    "In PyTorch, the **autograd engine** tracks operations performed on tensors so it can automatically compute gradients.  \n",
    "By default, tensors are created with `requires_grad=False`.  \n",
    "To enable gradient tracking, we set:  \n",
    "\n",
    "```python\n",
    "x = torch.tensor(2.0, requires_grad=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d848f394",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "requires_grad for a: False\n",
      "requires_grad for b: True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Tensor without autograd\n",
    "a = torch.tensor(2.0)\n",
    "print(\"requires_grad for a:\", a.requires_grad)\n",
    "\n",
    "# Tensor with autograd enabled\n",
    "b = torch.tensor(2.0, requires_grad=True)\n",
    "print(\"requires_grad for b:\", b.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a1a6811",
   "metadata": {},
   "source": [
    "<a name='2'></a>\n",
    "## 2 - Building a Computational Graph  \n",
    "\n",
    "When we perform operations on tensors with `requires_grad=True`, PyTorch dynamically builds a **computational graph**.  \n",
    "- Each node in the graph represents a tensor.  \n",
    "- Each edge represents a function that produced the tensor.  \n",
    "- PyTorch uses this graph to compute gradients during backpropagation.  \n",
    "\n",
    "The graph is built **dynamically**, meaning it is created as operations are executed, and discarded after `.backward()` (unless `retain_graph=True` is specified).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "31948a32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(16., grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor(3.0, requires_grad=True)\n",
    "y = x**2 + 2*x + 1   # A simple quadratic function\n",
    "print(y)\n",
    "# x is a leaf node (user-created tensor).\n",
    "\n",
    "# y is a result tensor computed from x.\n",
    "\n",
    "# Autograd has built a graph that knows how to compute dy/dx."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a8a0883",
   "metadata": {},
   "source": [
    "<a name='3'></a>\n",
    "## 3 - Computing Gradients with `.backward()`  \n",
    "\n",
    "Once a computational graph is built, we can compute derivatives using the `.backward()` method.  \n",
    "PyTorch applies the **chain rule of calculus** to propagate gradients back through the graph.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "94651fad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y: 16.0\n",
      "dy/dx at x=3: 8.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Create a tensor with autograd enabled\n",
    "x = torch.tensor(3.0, requires_grad=True)\n",
    "\n",
    "# Define a function y = x^2 + 2x + 1\n",
    "y = x**2 + 2*x + 1\n",
    "print(\"y:\", y.item())\n",
    "\n",
    "# Compute dy/dx\n",
    "y.backward()\n",
    "\n",
    "# Gradient stored in x.grad\n",
    "print(\"dy/dx at x=3:\", x.grad.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04630b0e",
   "metadata": {},
   "source": [
    "<a name='4'></a>\n",
    "## 4 - Accessing and Resetting Gradients  \n",
    "\n",
    "After calling `.backward()`, the computed gradients are stored in the `.grad` attribute of each leaf tensor.  \n",
    "However, **gradients accumulate by default** in PyTorch. This means if you call `.backward()` multiple times without clearing them, the values will add up.  \n",
    "\n",
    "To avoid this, we must **reset gradients** after each iteration, usually with:  \n",
    "```python\n",
    "x.grad.zero_()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "58adb7a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient after first backward: 4.0\n",
      "Gradient after second backward (accumulated): 7.0\n",
      "Gradient after reset: 0.0\n",
      "Gradient after second backward (fresh): 3.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Create a tensor with gradient tracking\n",
    "x = torch.tensor(2.0, requires_grad=True)\n",
    "\n",
    "# First function: y1 = x^2\n",
    "y1 = x**2\n",
    "y1.backward()\n",
    "print(\"Gradient after first backward:\", x.grad.item())\n",
    "\n",
    "# Without resetting, compute gradient again with y2 = 3x\n",
    "y2 = 3*x\n",
    "y2.backward()\n",
    "print(\"Gradient after second backward (accumulated):\", x.grad.item())\n",
    "\n",
    "# Reset gradients to zero\n",
    "x.grad.zero_()\n",
    "print(\"Gradient after reset:\", x.grad.item())\n",
    "\n",
    "# Recompute y2 = 3x\n",
    "y2 = 3*x\n",
    "y2.backward()\n",
    "print(\"Gradient after second backward (fresh):\", x.grad.item())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67c2f7ef",
   "metadata": {},
   "source": [
    "<a name='5'></a>\n",
    "## 5 - Example: Backpropagation in Action  \n",
    "\n",
    "Autograd can handle **functions with multiple inputs and outputs**.  \n",
    "Let’s see an example where gradients are computed with respect to two variables.  \n",
    "\n",
    "### Example  \n",
    "\n",
    "We define the function:  \n",
    "\n",
    "$$\n",
    "z = x \\cdot y + y^2\n",
    "$$  \n",
    "\n",
    "and compute gradients with respect to $x$ and $y$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f0bbdda7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "z: 15.0\n",
      "dz/dx at (x=2,y=3): 3.0\n",
      "dz/dy at (x=2,y=3): 8.0\n"
     ]
    }
   ],
   "source": [
    "# Create tensors with autograd enabled\n",
    "x = torch.tensor(2.0, requires_grad=True)\n",
    "y = torch.tensor(3.0, requires_grad=True)\n",
    "\n",
    "# Define a function z = x*y + y^2\n",
    "z = x * y + y**2\n",
    "print(\"z:\", z.item())\n",
    "\n",
    "# Backpropagation\n",
    "z.backward()\n",
    "\n",
    "# Gradients\n",
    "print(\"dz/dx at (x=2,y=3):\", x.grad.item())  # ∂z/∂x = y = 3\n",
    "print(\"dz/dy at (x=2,y=3):\", y.grad.item())  # ∂z/∂y = x + 2y = 2 + 6 = 8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74a1b14a",
   "metadata": {},
   "source": [
    "<a name='6'></a>\n",
    "## 6 - Practice Exercises  \n",
    "\n",
    "Now it’s your turn! Try the following exercises to reinforce your understanding of **Autograd**.  \n",
    "\n",
    "---\n",
    "\n",
    "### **Exercise 1: Single Variable Gradient**  \n",
    "Define the function:  \n",
    "\n",
    "$$\n",
    "y = 3x^3 - 4x\n",
    "$$  \n",
    "\n",
    "- Create a tensor $x = 2.0$ with `requires_grad=True`.  \n",
    "- Compute $y$ and use `.backward()` to get $\\frac{dy}{dx}$.  \n",
    "- Verify by hand:  \n",
    "\n",
    "$$\n",
    "\\frac{dy}{dx} = 9x^2 - 4\n",
    "$$  \n",
    "\n",
    "---\n",
    "\n",
    "### **Exercise 2: Two Variables Function**  \n",
    "Define the function:  \n",
    "\n",
    "$$\n",
    "z = x^2 \\cdot y + 5y\n",
    "$$  \n",
    "\n",
    "- Create $x = 1.0$, $y = 2.0$ with gradients enabled.  \n",
    "- Compute $\\frac{\\partial z}{\\partial x}$ and $\\frac{\\partial z}{\\partial y}$.  \n",
    "\n",
    "---\n",
    "\n",
    "### **Exercise 3: Gradient Reset**  \n",
    "- Create $x = 3.0$ with `requires_grad=True`.  \n",
    "- Define $y = 2x^2$.  \n",
    "- Call `.backward()` **twice** without resetting gradients.  \n",
    "- Observe what happens to $x.grad$.  \n",
    "- Then reset gradients with `x.grad.zero_()` and recompute.  \n",
    "\n",
    "---\n",
    "\n",
    "### **Challenge (Optional)**  \n",
    "Define a function of three variables:  \n",
    "\n",
    "$$\n",
    "f(x,y,z) = x \\cdot y + y \\cdot z + x \\cdot z\n",
    "$$  \n",
    "\n",
    "- Compute $\\frac{\\partial f}{\\partial x}$, $\\frac{\\partial f}{\\partial y}$, and $\\frac{\\partial f}{\\partial z}$ using autograd.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0708bbf8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
