{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a84d75e7",
   "metadata": {},
   "source": [
    "<div style=\"text-align:left;\">\n",
    "  <a href=\"https://code213.tech/\" target=\"_blank\">\n",
    "    <img src=\"../code213.PNG\" alt=\"code213\">\n",
    "  </a>\n",
    "  <p><em>prepared by Latreche Sara</em></p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "361e8147",
   "metadata": {},
   "source": [
    "# Data Handling in PyTorch: Dataset & DataLoader\n",
    "\n",
    "Efficiently handling data is a **critical step** in training deep learning models.  \n",
    "\n",
    "PyTorch provides two key components:  \n",
    "\n",
    "1. **Dataset**  \n",
    "   - Encapsulates the data and labels.  \n",
    "   - Must implement:\n",
    "     - `__len__()` → returns the number of samples  \n",
    "     - `__getitem__()` → returns a single sample  \n",
    "\n",
    "2. **DataLoader**  \n",
    "   - Wraps a `Dataset` to provide:\n",
    "     - Mini-batches  \n",
    "     - Shuffling  \n",
    "     - Parallel loading using multiple workers  \n",
    "\n",
    "\n",
    "\n",
    "In this notebook, we will cover:  \n",
    "- Creating custom datasets with `torch.utils.data.Dataset`  \n",
    "- Using `DataLoader` to iterate over batches  \n",
    "- Handling shuffling, batch size, and parallelism  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2528e74e",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "- [1 - PyTorch Dataset](#1)\n",
    "- [2 - Custom Dataset](#2)\n",
    "- [3 - DataLoader](#3)\n",
    "- [4 - Iterating through DataLoader](#4)\n",
    "- [5 - Transformations & Normalization](#5)\n",
    "- [6 - Practice Exercises](#6)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c61b9645",
   "metadata": {},
   "source": [
    "<a name='1'></a>\n",
    "## 1 - PyTorch Dataset\n",
    "\n",
    "A `Dataset` in PyTorch is an **abstract class** representing a collection of data samples and their corresponding labels.  \n",
    "\n",
    "Key points:  \n",
    "- Every dataset must implement two methods:\n",
    "  1. `__len__()` → returns the number of samples in the dataset  \n",
    "  2. `__getitem__(index)` → returns a sample and its label at the given index  \n",
    "- Allows **flexible and efficient data access**  \n",
    "- Can be used with `DataLoader` to create batches and shuffle data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a48a6f28",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "847b2871",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First sample: tensor([-0.0363,  0.9612]) tensor(0)\n",
      "Dataset length: 5\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "# Example: simple dataset\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self):\n",
    "        # Sample data: 5 samples, 2 features\n",
    "        self.x = torch.randn(5, 2)\n",
    "        self.y = torch.tensor([0, 1, 0, 1, 0])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.x[idx], self.y[idx]\n",
    "\n",
    "# Create dataset instance\n",
    "dataset = MyDataset()\n",
    "\n",
    "# Accessing a sample\n",
    "sample_x, sample_y = dataset[0]\n",
    "print(\"First sample:\", sample_x, sample_y)\n",
    "print(\"Dataset length:\", len(dataset))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "185711f5",
   "metadata": {},
   "source": [
    "<a name='2'></a>\n",
    "## 2 - Custom Dataset\n",
    "\n",
    "Creating a **custom dataset** allows you to handle:  \n",
    "- Your own data files (CSV, images, etc.)  \n",
    "- Preprocessing and transformations  \n",
    "- Lazy loading (loading samples on the fly)  \n",
    "\n",
    "Key steps to create a custom dataset:  \n",
    "1. Inherit from `torch.utils.data.Dataset`  \n",
    "2. Implement `__init__()`, `__len__()`, and `__getitem__()`  \n",
    "3. Optionally, add **transformations** for preprocessing  \n",
    "\n",
    "This approach is flexible and works with `DataLoader` to create batches and shuffle data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "53ed3eb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 1: tensor([ 0.9238, -1.9255,  1.5385]) tensor(1)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data, labels, transform=None):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        x = self.data[idx]\n",
    "        y = self.labels[idx]\n",
    "        \n",
    "        # Apply optional transformation\n",
    "        if self.transform:\n",
    "            x = self.transform(x)\n",
    "        \n",
    "        return x, y\n",
    "\n",
    "# Example data\n",
    "data = torch.randn(6, 3)\n",
    "labels = torch.tensor([0, 1, 0, 1, 0, 1])\n",
    "\n",
    "# Create dataset\n",
    "dataset = CustomDataset(data, labels)\n",
    "\n",
    "# Access a sample\n",
    "x_sample, y_sample = dataset[1]\n",
    "print(\"Sample 1:\", x_sample, y_sample)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afdb2b62",
   "metadata": {},
   "source": [
    "<a name='3'></a>\n",
    "## 3 - DataLoader\n",
    "\n",
    "`DataLoader` in PyTorch provides an **iterable over a Dataset** with many convenient features:\n",
    "\n",
    "### Key Features\n",
    "- **Mini-batches**: Split dataset into smaller batches for efficient training  \n",
    "- **Shuffling**: Randomly shuffle data at the start of each epoch  \n",
    "- **Parallel loading**: Use multiple workers to speed up data loading  \n",
    "- **Custom collate function**: Control how samples are combined into batches  \n",
    "\n",
    "### Common Parameters\n",
    "- `dataset`: Dataset object to load from  \n",
    "- `batch_size`: Number of samples per batch  \n",
    "- `shuffle`: Whether to shuffle data each epoch  \n",
    "- `num_workers`: Number of subprocesses to use for data loading  \n",
    "\n",
    "DataLoader makes it easy to **iterate over mini-batches** in a training loop.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2d797985",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1\n",
      "x: tensor([[ 0.6115, -1.0577],\n",
      "        [ 0.9681, -0.5553],\n",
      "        [-1.1261,  2.0470]])\n",
      "y: tensor([0, 0, 0])\n",
      "---\n",
      "Batch 2\n",
      "x: tensor([[ 1.4703,  0.9640],\n",
      "        [ 0.3942,  1.7599],\n",
      "        [ 0.7142, -1.2718]])\n",
      "y: tensor([1, 1, 1])\n",
      "---\n",
      "Batch 3\n",
      "x: tensor([[ 0.7942, -0.3868],\n",
      "        [ 0.5389,  1.4670],\n",
      "        [ 0.3613,  0.4697]])\n",
      "y: tensor([0, 1, 1])\n",
      "---\n",
      "Batch 4\n",
      "x: tensor([[-0.3986, -0.3414]])\n",
      "y: tensor([1])\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Sample dataset\n",
    "x = torch.randn(10, 2)\n",
    "y = torch.randint(0, 2, (10,))\n",
    "dataset = TensorDataset(x, y)\n",
    "\n",
    "# Create DataLoader\n",
    "dataloader = DataLoader(dataset, batch_size=3, shuffle=True, num_workers=0)\n",
    "\n",
    "# Iterate through batches\n",
    "for batch_idx, (x_batch, y_batch) in enumerate(dataloader):\n",
    "    print(f\"Batch {batch_idx+1}\")\n",
    "    print(\"x:\", x_batch)\n",
    "    print(\"y:\", y_batch)\n",
    "    print(\"---\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f633440",
   "metadata": {},
   "source": [
    "<a name='4'></a>\n",
    "## 4 - Iterating through DataLoader\n",
    "\n",
    "Once a `DataLoader` is created, you can **iterate over it in a training loop** to process mini-batches.  \n",
    "\n",
    "### Key Points\n",
    "- Each iteration returns a batch of inputs and labels.  \n",
    "- Combine with **forward pass, loss computation, and backward pass** during training.  \n",
    "- Shuffling ensures batches are different each epoch, which helps prevent overfitting.  \n",
    "\n",
    "Typical pattern:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c4722345",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "Batch 1, Loss: 0.3627\n",
      "Batch 2, Loss: 0.6184\n",
      "Batch 3, Loss: 0.8799\n",
      "Batch 4, Loss: 1.1342\n",
      "Epoch 2\n",
      "Batch 1, Loss: 0.8160\n",
      "Batch 2, Loss: 0.4689\n",
      "Batch 3, Loss: 0.5183\n",
      "Batch 4, Loss: 1.1568\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Sample dataset\n",
    "x = torch.randn(8, 2)\n",
    "y = torch.randint(0, 2, (8,))\n",
    "dataset = TensorDataset(x, y)\n",
    "dataloader = DataLoader(dataset, batch_size=2, shuffle=True)\n",
    "\n",
    "# Simple model\n",
    "model = nn.Linear(2, 2)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "# Iterate through DataLoader\n",
    "for epoch in range(2):\n",
    "    print(f\"Epoch {epoch+1}\")\n",
    "    for batch_idx, (x_batch, y_batch) in enumerate(dataloader):\n",
    "        outputs = model(x_batch)\n",
    "        loss = loss_fn(outputs, y_batch)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        print(f\"Batch {batch_idx+1}, Loss: {loss.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "749c6420",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5f32b488",
   "metadata": {},
   "source": [
    "<a name='5'></a>\n",
    "## 5 - Transformations & Normalization\n",
    "\n",
    "Transformations allow you to **preprocess data** before feeding it to a model.  \n",
    "Common transformations include:\n",
    "\n",
    "1. **Normalization**  \n",
    "   - Scale features to a standard range (e.g., 0–1 or mean=0, std=1)  \n",
    "   - Helps training converge faster\n",
    "\n",
    "2. **Type conversion**  \n",
    "   - Convert images to tensors (`torchvision.transforms.ToTensor`)  \n",
    "   - Convert data to float32 if needed  \n",
    "\n",
    "3. **Data augmentation** (for images)  \n",
    "   - Random rotations, flips, crops  \n",
    "   - Improves model generalization  \n",
    "\n",
    "PyTorch provides **`torchvision.transforms`** for image datasets.  \n",
    "For custom datasets, you can define a `transform` function and apply it in `__getitem__()`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "42d93819",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: tensor([[ 0.3140, -1.1193,  0.8053],\n",
      "        [ 0.0439,  0.9773, -1.0212]])\n",
      "y: tensor([0, 1])\n",
      "---\n",
      "x: tensor([[-1.1143,  0.2951,  0.8192],\n",
      "        [ 0.0331,  0.9830, -1.0161]])\n",
      "y: tensor([1, 0])\n",
      "---\n",
      "x: tensor([[ 0.7484, -1.1357,  0.3874]])\n",
      "y: tensor([1])\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Sample transformation: normalize features\n",
    "def normalize(x):\n",
    "    return (x - x.mean()) / x.std()\n",
    "\n",
    "class TransformDataset(Dataset):\n",
    "    def __init__(self, data, labels, transform=None):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        x = self.data[idx]\n",
    "        y = self.labels[idx]\n",
    "        if self.transform:\n",
    "            x = self.transform(x)\n",
    "        return x, y\n",
    "\n",
    "# Sample data\n",
    "data = torch.randn(5, 3)\n",
    "labels = torch.randint(0, 2, (5,))\n",
    "\n",
    "dataset = TransformDataset(data, labels, transform=normalize)\n",
    "dataloader = DataLoader(dataset, batch_size=2, shuffle=True)\n",
    "\n",
    "# Iterate through transformed data\n",
    "for x_batch, y_batch in dataloader:\n",
    "    print(\"x:\", x_batch)\n",
    "    print(\"y:\", y_batch)\n",
    "    print(\"---\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d162e47",
   "metadata": {},
   "source": [
    "<a name='6'></a>\n",
    "## 6 - Practice Exercises\n",
    "\n",
    "Try the following exercises to reinforce your understanding of **Dataset, DataLoader, and Transformations**:\n",
    "\n",
    "\n",
    "\n",
    "### **Exercise 1: Simple Dataset**\n",
    "- Create a custom Dataset with 10 samples, each with 3 features.  \n",
    "- Return the sample and a label (0 or 1).  \n",
    "- Print the first sample and its label.\n",
    "\n",
    "\n",
    "\n",
    "### **Exercise 2: DataLoader**\n",
    "- Wrap the dataset with a DataLoader.  \n",
    "- Set `batch_size=4` and `shuffle=True`.  \n",
    "- Iterate through all batches and print them.\n",
    "\n",
    "\n",
    "### **Exercise 3: Transformation**\n",
    "- Add a transformation to normalize each sample:  \n",
    "\n",
    "$$\n",
    "x_{\\text{norm}} = \\frac{x - \\text{mean}(x)}{\\text{std}(x)}\n",
    "$$  \n",
    "\n",
    "- Print the normalized batches from the DataLoader.\n",
    "\n",
    "\n",
    "\n",
    "### **Exercise 4 (Optional): Mini-Batch Training**\n",
    "- Use a simple linear model with input size 3 → output size 1.  \n",
    "- Use MSE loss and SGD optimizer.  \n",
    "- Iterate through the DataLoader for 2 epochs and print batch loss.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "119bbb33",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d05abc30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First sample: (tensor([-0.2774, -0.5221,  0.9460]), tensor(1))\n",
      "Batch 1\n",
      "x: tensor([[-1.4489,  0.2493,  0.4823],\n",
      "        [ 0.8521, -0.1315,  0.5867],\n",
      "        [ 1.1370,  0.2952, -0.1801],\n",
      "        [-1.5567,  2.1936,  0.7252]])\n",
      "y: tensor([0, 0, 0, 0])\n",
      "---\n",
      "Batch 2\n",
      "x: tensor([[-1.9914,  0.7881,  0.2722],\n",
      "        [ 0.7671,  0.3641,  1.2196],\n",
      "        [-0.6737, -0.7288,  0.2280],\n",
      "        [ 0.3095,  0.5873, -0.1617]])\n",
      "y: tensor([0, 0, 0, 0])\n",
      "---\n",
      "Batch 3\n",
      "x: tensor([[-0.2774, -0.5221,  0.9460],\n",
      "        [-1.6754, -1.6615,  0.0034]])\n",
      "y: tensor([1, 1])\n",
      "---\n",
      "Normalized batch x: tensor([[-0.4147, -0.7259,  1.1406],\n",
      "        [-0.0386, -0.9801,  1.0188],\n",
      "        [-1.1476,  0.4633,  0.6843],\n",
      "        [-1.0640,  0.9205,  0.1435]])\n",
      "y: tensor([1, 0, 0, 0])\n",
      "---\n",
      "Normalized batch x: tensor([[ 0.1702,  0.9040, -1.0742],\n",
      "        [-0.5253, -0.6279,  1.1532],\n",
      "        [ 1.0789, -0.1832, -0.8958],\n",
      "        [-1.1370,  0.7430,  0.3940]])\n",
      "y: tensor([0, 0, 0, 0])\n",
      "---\n",
      "Normalized batch x: tensor([[ 0.8182, -1.1147,  0.2965],\n",
      "        [-0.5846, -0.5701,  1.1547]])\n",
      "y: tensor([0, 1])\n",
      "---\n",
      "Epoch 1, Loss: 0.3237\n",
      "Epoch 1, Loss: 0.3485\n",
      "Epoch 1, Loss: 0.4903\n",
      "Epoch 2, Loss: 0.3514\n",
      "Epoch 2, Loss: 0.4671\n",
      "Epoch 2, Loss: 0.0262\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# ----------------------------\n",
    "# Exercise 1: Simple Dataset\n",
    "# ----------------------------\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self):\n",
    "        self.data = torch.randn(10, 3)\n",
    "        self.labels = torch.randint(0, 2, (10,))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.labels[idx]\n",
    "\n",
    "dataset = MyDataset()\n",
    "print(\"First sample:\", dataset[0])\n",
    "\n",
    "# ----------------------------\n",
    "# Exercise 2: DataLoader\n",
    "# ----------------------------\n",
    "dataloader = DataLoader(dataset, batch_size=4, shuffle=True)\n",
    "for batch_idx, (x_batch, y_batch) in enumerate(dataloader):\n",
    "    print(f\"Batch {batch_idx+1}\")\n",
    "    print(\"x:\", x_batch)\n",
    "    print(\"y:\", y_batch)\n",
    "    print(\"---\")\n",
    "\n",
    "# ----------------------------\n",
    "# Exercise 3: Transformation\n",
    "# ----------------------------\n",
    "def normalize(x):\n",
    "    return (x - x.mean()) / x.std()\n",
    "\n",
    "class TransformDataset(Dataset):\n",
    "    def __init__(self, data, labels, transform=None):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        x = self.data[idx]\n",
    "        y = self.labels[idx]\n",
    "        if self.transform:\n",
    "            x = self.transform(x)\n",
    "        return x, y\n",
    "\n",
    "trans_dataset = TransformDataset(dataset.data, dataset.labels, transform=normalize)\n",
    "trans_loader = DataLoader(trans_dataset, batch_size=4, shuffle=True)\n",
    "\n",
    "for x_batch, y_batch in trans_loader:\n",
    "    print(\"Normalized batch x:\", x_batch)\n",
    "    print(\"y:\", y_batch)\n",
    "    print(\"---\")\n",
    "\n",
    "# ----------------------------\n",
    "# Exercise 4: Mini-Batch Training\n",
    "# ----------------------------\n",
    "model = nn.Linear(3, 1)\n",
    "loss_fn = nn.MSELoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "for epoch in range(2):\n",
    "    for x_batch, y_batch in trans_loader:\n",
    "        y_batch = y_batch.float().unsqueeze(1)\n",
    "        outputs = model(x_batch)\n",
    "        loss = loss_fn(outputs, y_batch)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}, Loss: {loss.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f36d6bd5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
