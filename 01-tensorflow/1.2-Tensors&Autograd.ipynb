{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3de750ea",
   "metadata": {},
   "source": [
    "<div style=\"text-align:left;\">\n",
    "  <a href=\"https://code213.tech/\" target=\"_blank\">\n",
    "    <img src=\"code213.PNG\" alt=\"Code213 Logo\" width=\"200\"/>\n",
    "  </a>\n",
    "  <p><em>Prepared by Latreche Sara</em></p>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "080ae56c",
   "metadata": {},
   "source": [
    "# 2.0 â€” Tensors & Gradients\n",
    "<img src=\"https://www.tensorflow.org/images/tf_logo_social.png\" alt=\"TensorFlow Logo\" width=\"200\"/>\n",
    "\n",
    "**What are Gradients?**  \n",
    "\n",
    "Gradients represent the **rate of change of a function with respect to its inputs**.  \n",
    "In deep learning, gradients are used to **update model parameters** via optimization algorithms like **Gradient Descent**.  \n",
    "\n",
    "TensorFlow provides **automatic differentiation** using `tf.GradientTape`, making it easy to compute gradients for any differentiable computation.\n",
    "## Table of Contents  \n",
    "\n",
    "- [1 - Packages](#1)  \n",
    "- [2 - Outline of the Notebook](#2)  \n",
    "- [3 - Creating Tensors](#3)  \n",
    "- [4 - Computing Gradients with `GradientTape`](#4)  \n",
    "  - [4.1 - Gradient of simple functions](#4-1)  \n",
    "  - [4.2 - Gradient of vector functions](#4-2)  \n",
    "- [5 - Training Example with Gradient Descent](#5)  \n",
    "- [6 - Exercises](#6)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c50ffad6",
   "metadata": {},
   "source": [
    "## 1 - Packages <a name=\"1\"></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "227ca269",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e76213dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fa70837",
   "metadata": {},
   "source": [
    "## 2 - Outline of the Notebook <a name=\"2\"></a>\n",
    "\n",
    "This notebook covers:  \n",
    "\n",
    "1. Creating tensors for computations  \n",
    "2. Using `tf.GradientTape` to compute gradients  \n",
    "3. Understanding gradient flow for scalar and vector functions  \n",
    "4. Implementing a small training example using gradient descent  \n",
    "5. Exercises for practice\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2041d01d",
   "metadata": {},
   "source": [
    "## 3 - Creating Tensors <a name=\"3\"></a>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2b7ae0da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scalar x: <tf.Variable 'Variable:0' shape=() dtype=float32, numpy=3.0>\n",
      "Vector y: <tf.Variable 'Variable:0' shape=(3,) dtype=float32, numpy=array([1., 2., 3.], dtype=float32)>\n"
     ]
    }
   ],
   "source": [
    "# Scalar tensor\n",
    "x = tf.Variable(3.0)\n",
    "\n",
    "# Vector tensor\n",
    "y = tf.Variable([1.0, 2.0, 3.0])\n",
    "\n",
    "print(\"Scalar x:\", x)\n",
    "print(\"Vector y:\", y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46d29210",
   "metadata": {},
   "source": [
    "## 4 - Computing Gradients with `GradientTape` <a name=\"4\"></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b56882b8",
   "metadata": {},
   "source": [
    "### 4.1 - Gradient of simple functions <a name=\"4-1\"></a>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3c06c716",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient df/dx at x=3: tf.Tensor(6.0, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# Define a simple function: f(x) = x^2\n",
    "x = tf.Variable(3.0)\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "    f = x ** 2\n",
    "\n",
    "# Compute the gradient df/dx\n",
    "grad = tape.gradient(f, x)\n",
    "print(\"Gradient df/dx at x=3:\", grad)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac7a0003",
   "metadata": {},
   "source": [
    "### 4.2 - Gradient of vector functions <a name=\"4-2\"></a>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "35af8065",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient df/dy: tf.Tensor([2. 4. 6.], shape=(3,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# Vector function: f(y) = y1^2 + y2^2 + y3^2\n",
    "y = tf.Variable([1.0, 2.0, 3.0])\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "    f = tf.reduce_sum(y ** 2)\n",
    "\n",
    "grad = tape.gradient(f, y)\n",
    "print(\"Gradient df/dy:\", grad)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b42d54e6",
   "metadata": {},
   "source": [
    "## 5 - Training Example with Gradient Descent <a name=\"5\"></a>\n",
    "\n",
    "Let's minimize the function f(x) = x^2 using **gradient descent**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79728d2e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4bffa3b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: x=4.0, f(x)=25.0\n",
      "Step 2: x=3.200000047683716, f(x)=16.0\n",
      "Step 3: x=2.559999942779541, f(x)=10.24000072479248\n",
      "Step 4: x=2.047999858856201, f(x)=6.553599834442139\n",
      "Step 5: x=1.6383998394012451, f(x)=4.194303512573242\n",
      "Step 6: x=1.3107198476791382, f(x)=2.684354066848755\n",
      "Step 7: x=1.0485758781433105, f(x)=1.7179864645004272\n",
      "Step 8: x=0.8388606905937195, f(x)=1.0995113849639893\n",
      "Step 9: x=0.6710885763168335, f(x)=0.7036872506141663\n",
      "Step 10: x=0.5368708372116089, f(x)=0.45035988092422485\n"
     ]
    }
   ],
   "source": [
    "# Initialize variable\n",
    "x = tf.Variable(5.0)\n",
    "\n",
    "# Learning rate\n",
    "lr = 0.1\n",
    "\n",
    "# Gradient descent loop\n",
    "for i in range(10):\n",
    "    with tf.GradientTape() as tape:\n",
    "        f = x ** 2\n",
    "    grad = tape.gradient(f, x)\n",
    "    x.assign_sub(lr * grad)\n",
    "    print(f\"Step {i+1}: x={x.numpy()}, f(x)={f.numpy()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c840117d",
   "metadata": {},
   "source": [
    "## 6 - Exercises <a name=\"6\"></a>\n",
    "\n",
    "1. Compute the gradient of f(x) = 3x^3 + 2x at x=2.  \n",
    "2. Compute the gradient of f(y) = y1*y2 + y2*y3 for y = [1.0, 2.0, 3.0].  \n",
    "3. Implement gradient descent to minimize f(x) = (x-4)^2 starting from x=0 with learning rate 0.2 for 15 steps.  \n",
    "4. Experiment with a 2-variable function f(x,y) = x^2 + y^2 and compute gradients w.r.t both x and y.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3301c06",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
