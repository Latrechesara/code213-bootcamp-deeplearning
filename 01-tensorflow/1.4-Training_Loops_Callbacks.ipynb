{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "40287ab9",
   "metadata": {},
   "source": [
    "<div style=\"text-align:left;\">\n",
    "  <a href=\"https://code213.tech/\" target=\"_blank\">\n",
    "    <img src=\"code213.PNG\" alt=\"Code213 Logo\" width=\"200\"/>\n",
    "  </a>\n",
    "  <p><em>Prepared by Latreche Sara</em></p>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a226dd1",
   "metadata": {},
   "source": [
    "# 4.0 — Training Loops & Callbacks\n",
    "<img src=\"https://www.tensorflow.org/images/tf_logo_social.png\" alt=\"TensorFlow Logo\" width=\"200\"/>\n",
    "\n",
    "**What are Training Loops and Callbacks?**  \n",
    "\n",
    "- **Training loops** are used to optimize model parameters with respect to a loss function.  \n",
    "- TensorFlow provides **high-level API** (`model.fit`) for training.  \n",
    "- For custom behavior, you can use **low-level training loops** with `tf.GradientTape`.  \n",
    "- **Callbacks** allow adding functionalities during training like:\n",
    "  - Early stopping\n",
    "  - Model checkpoints\n",
    "  - Learning rate scheduling\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26191ceb",
   "metadata": {},
   "source": [
    "## Table of Contents  \n",
    "\n",
    "- [1 - Packages](#1)  \n",
    "- [2 - Outline of the Notebook](#2)  \n",
    "- [3 - Sample Dataset](#3)  \n",
    "- [4 - High-level Training with `model.fit`](#4)  \n",
    "- [5 - Custom Training Loop](#5)  \n",
    "- [6 - Using Callbacks](#6)  \n",
    "- [7 - Exercises](#7)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b14dcd34",
   "metadata": {},
   "source": [
    "## 1 - Packages <a name=\"1\"></a>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "10eb8d9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, optimizers, losses\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "428a3be7",
   "metadata": {},
   "source": [
    "## 2 - Outline of the Notebook <a name=\"2\"></a>\n",
    "\n",
    "This notebook covers:  \n",
    "\n",
    "1. Creating a sample dataset  \n",
    "2. Training a model using `model.fit`  \n",
    "3. Implementing a custom training loop with `GradientTape`  \n",
    "4. Using callbacks to enhance training  \n",
    "5. Exercises for practice\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6ccb8794",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample dataset: y = 2x + 1\n",
    "x_train = np.array([[1.0], [2.0], [3.0], [4.0], [5.0]])\n",
    "y_train = np.array([[3.0], [5.0], [7.0], [9.0], [11.0]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be2cf65c",
   "metadata": {},
   "source": [
    "## 4 - High-level Training with `model.fit` <a name=\"4\"></a>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "516ca46f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\MatenTech\\AppData\\Local\\anaconda3\\envs\\bd\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:92: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 102ms/step\n",
      "Predictions:\n",
      " [[ 2.8244758]\n",
      " [ 4.891713 ]\n",
      " [ 6.9589505]\n",
      " [ 9.026188 ]\n",
      " [11.093426 ]]\n"
     ]
    }
   ],
   "source": [
    "# Build a simple model\n",
    "model = models.Sequential([\n",
    "    layers.Dense(1, input_shape=(1,))\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='sgd', loss='mse')\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(x_train, y_train, epochs=50, verbose=0)\n",
    "\n",
    "# Predictions\n",
    "predictions = model.predict(x_train)\n",
    "print(\"Predictions:\\n\", predictions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cf44da8",
   "metadata": {},
   "source": [
    "## 5 - Custom Training Loop <a name=\"5\"></a>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3661f2d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10: loss = 38079.5078125\n",
      "Epoch 20: loss = 19539648.0\n",
      "Epoch 30: loss = 10026321920.0\n",
      "Epoch 40: loss = 5144778375168.0\n",
      "Epoch 50: loss = 2639925539241984.0\n"
     ]
    }
   ],
   "source": [
    "# Reset variable\n",
    "model = models.Sequential([layers.Dense(1, input_shape=(1,))])\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=0.1)\n",
    "loss_fn = tf.keras.losses.MeanSquaredError()\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(50):\n",
    "    with tf.GradientTape() as tape:\n",
    "        y_pred = model(x_train, training=True)\n",
    "        loss = loss_fn(y_train, y_pred)\n",
    "    grads = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "    if (epoch+1) % 10 == 0:\n",
    "        print(f\"Epoch {epoch+1}: loss = {loss.numpy()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ebcb854",
   "metadata": {},
   "source": [
    "## 6 - Using Callbacks <a name=\"6\"></a>\n",
    "\n",
    "Callbacks allow us to perform actions during training, like stopping early or saving the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ab852661",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training finished with EarlyStopping.\n"
     ]
    }
   ],
   "source": [
    "# Early stopping callback\n",
    "early_stop = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='loss',\n",
    "    patience=5,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "# Train again with callback\n",
    "model.compile(optimizer='sgd', loss='mse')\n",
    "history = model.fit(x_train, y_train, epochs=50, verbose=0, callbacks=[early_stop])\n",
    "print(\"Training finished with EarlyStopping.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1af51929",
   "metadata": {},
   "source": [
    "## 7 - Exercises <a name=\"7\"></a>\n",
    "\n",
    "1. Modify the high-level training example to use 100 epochs and `adam` optimizer.  \n",
    "2. Implement a custom training loop for y = 3x - 2.  \n",
    "3. Add a ModelCheckpoint callback to save the model whenever the loss improves.  \n",
    "4. Experiment with learning rates: 0.01, 0.1, 0.5 and observe convergence.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5291e1cf",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
